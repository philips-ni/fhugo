#+hugo_base_dir: ../../
# -*- mode: org; coding: utf-8; -*-
* Header Information                                               :noexport:
#+LaTeX_CLASS_OPTIONS: [11pt]
#+LATEX_HEADER: \usepackage{helvetica}
#+LATEX_HEADER: \setlength{\textwidth}{5.1in} % set width of text portion
#+LATEX_HEADER: \usepackage{geometry}
#+TITLE:     AWS tips
#+AUTHOR:    Fei Ni
#+EMAIL:     fei.ni@helix.com
#+DATE:      2021-01-01
#+HUGO_CATEGORIES: helix
#+HUGO_tags: helix
#+hugo_auto_set_lastmod: t
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:   ^:{}
#+INFOJS_OPT: view:nil toc:nil ltoc:nil mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+HTML_HEAD: <link rel="stylesheet" href="org.css" type="text/css"/>
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:

#+STARTUP: hidestars

#+STARTUP: overview   (or: showall, content, showeverything)
http://orgmode.org/org.html#Visibility-cycling  info:org#Visibility cycling

#+TODO: TODO(t) NEXT(n) STARTED(s) WAITING(w@/!) SOMEDAY(S!) | DONE(d!/!) CANCELLED(c@/!)
http://orgmode.org/org.html#Per_002dfile-keywords  info:org#Per-file keywords

#+TAGS: important(i) private(p)
#+TAGS: @HOME(h) @OFFICE(o)
http://orgmode.org/org.html#Setting-tags  info:org#Setting tags

#+NOstartup: beamer
#+NOLaTeX_CLASS: beamer
#+NOLaTeX_CLASS_OPTIONS: [bigger]
#+NOBEAMER_FRAME_LEVEL: 2


# Start from here
* DNS

 - https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-domain-https/

* IAMS
 - https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html

* CDK
** cross account permission grant

Requirement:
#+begin_src bash
Athena service is running in master account,  
STT import lambda is running in hipaa-staging account

STT import lambda want to query data in Athena service.

#+end_src
 The idea is :
  - create a role in =master= account which defines the policy to allow to query =Athena=
  - in CDK of =STT import lambda= grant the permission to assume the role which is created in #1
  - In =STT import lambda= golang code, before querying =Athena=, running assumeRole to get the temporarily access  
  - https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html
  - https://aws.amazon.com/premiumsupport/knowledge-center/access-denied-athena/
*** About implementation of =assumeRole=


*** policy example
#+begin_src bash
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "athena:GetQueryExecution",
                "athena:StartQueryExecution",
                "athena:ListDatabases",
                "athena:ListPreparedStatements",
                "athena:ListNamedQueries"
            ],
            "Resource": "arn:aws:athena:us-east-1:820411415250:workgroup/primary"
        },
        {
            "Effect": "Allow",
            "Action": [
                "glue:GetTable"
            ],
            "Resource": [
                "arn:aws:glue:us-east-1:820411415250:catalog",
                "arn:aws:glue:us-east-1:820411415250:database/lims_db_staging",
                "arn:aws:glue:us-east-1:820411415250:table/lims_db_staging/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "athena:ListDataCatalogs",
                "athena:ListWorkGroups"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListAllMyBuckets",
                "s3:GetBucketLocation"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:List*"
            ],
            "Resource": [
                "arn:aws:s3:::helix-lims-backup",
                "arn:aws:s3:::aws-athena-query-results-820411415250-us-east-1"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::helix-lims-backup/staging/*",
                "arn:aws:s3:::aws-athena-query-results-820411415250-us-east-1/staging/*"
            ]
        }
    ]
}
#+end_src
* ECR
** ECR login

#+begin_src bash

[fei.ni@fei-ni-C02FG3R2MD6N-SM master-dev helix-py-app-r2v (master %)]$ cat ecr_login
aws ecr get-login-password \
        --region us-east-1 | docker login \
                --username AWS \
                        --password-stdin 820411415250.dkr.ecr.us-east-1.amazonaws.com
#+end_src
* Redshift & Glue
 - https://aws.amazon.com/blogs/big-data/analyze-your-amazon-s3-spend-using-aws-glue-and-amazon-redshift/
 - https://hevodata.com/learn/spark-vs-redshift/
 - https://aws.amazon.com/blogs/big-data/introducing-aws-glue-3-0-with-optimized-apache-spark-3-1-runtime-for-faster-data-integration/
* Step functions
 - https://randomwits.com/blog/tutorial-cdk-aws
** example created by sfn.StateMachine
#+begin_src bash
{
  "StartAt": "prepareBatchJobInputTask",
  "States": {
    "prepareBatchJobInputTask": {
      "Next": "submitBatchJobs",
      "Retry": [
        {
          "ErrorEquals": [
            "Lambda.ServiceException",
            "Lambda.AWSLambdaException",
            "Lambda.SdkClientException"
          ],
          "IntervalSeconds": 2,
          "MaxAttempts": 6,
          "BackoffRate": 2
        }
      ],
      "Type": "Task",
      "ResultPath": "$.JobInput",
      "Resource": "arn:aws:states:::lambda:invoke",
      "Parameters": {
        "FunctionName": "arn:aws:lambda:us-east-1:409670809604:function:HipaaAnalysisWorkflowVseq-PrepareBatchJobInputLamb-TTuXLHMdHRWv",
        "Payload.$": "$"
      }
    },
    "submitBatchJobs": {
      "Next": "runGlueTriggerFastagenerator",
      "Type": "Task",
      "InputPath": "$.JobInput",
      "Resource": "arn:aws:states:::batch:submitJob.sync",
      "Parameters": {
        "JobDefinition": "arn:aws:batch:us-east-1:409670809604:job-definition/helix-sars-klados-batch-job:3",
        "JobName": "$.JobName",
        "JobQueue": "arn:aws:batch:us-east-1:409670809604:job-queue/helix-sars-klados-batch-job-queue",
        "ArrayProperties": {
          "Size": 1152
        },
        "ContainerOverrides": {
          "Environment": [
            {
              "Name": "INPUT_S3_FOLDER",
              "Value": "$.JobS3InputFolder"
            },
            {
              "Name": "TRIM",
              "Value": "$.JobTrimSetting"
            }
          ]
        },
        "RetryStrategy": {
          "Attempts": 3
        }
      }
    },
    "runGlueTriggerFastagenerator": {
      "Next": "runGlueTriggerclassifierPart1",
      "Type": "Task",
      "Resource": "arn:aws:states:::glue:startJobRun",
      "Parameters": {
        "JobName": "research-etl-trigger-incremental-helix-seq-metrics-on-demand"
      }
    },
    "runGlueTriggerclassifierPart1": {
      "Next": "runGlueTriggerclassifierPart2",
      "Type": "Task",
      "Resource": "arn:aws:states:::glue:startJobRun",
      "Parameters": {
        "JobName": "research-etl-trigger-klados-on-demand"
      }
    },
    "runGlueTriggerclassifierPart2": {
      "End": true,
      "Type": "Task",
      "Resource": "arn:aws:states:::glue:startJobRun",
      "Parameters": {
        "JobName": "research-etl-trigger-klados-on-demand-pt2"
      }
    }
  }
}
#+end_src

#+begin_src bash
    const definitionStr = `
{
  "StartAt": "prepareBatchJobInputTask",
  "States": {
    "prepareBatchJobInputTask": {
      "Next": "submitBatchJobs",
      "Type": "Task",
      "InputPath": "$",
      "ResultPath": "$.JobInput",
      "Resource": "arn:aws:states:::lambda:invoke",
      "Parameters": {
        "FunctionName": "arn:aws:lambda:us-east-1:XXX",
        "Payload.$": "$"
      }
    },
  ...
`
 
    new sfn.CfnStateMachine(this, 'VseqStepFunction', {
      stateMachineType: 'STANDARD',
      roleArn: 'arn:aws:iam::' + cdk.Aws.ACCOUNT_ID + ':role/' + stepFunctionRole.value,
      definitionString: definitionStr,
      tags: [
        {
          key: 'environment',
          value: this.namedEnv.name,
        },
      ],
    });


#+end_src
* Python library for DynamoDB
 - https://highlandsolutions.com/blog/hands-on-examples-for-working-with-dynamodb-boto3-and-python
** atomic counter:
   #+begin_src python
     # {    "siteUrl": "https://www.linuxacademy.com/",    "visits": "0"}
     import boto3d = boto3.client('dynamodb')
     response = dynamodb.update_item(
	 TableName='siteVisits',
	 Key={ siteUrl':{'S': "https://www.linuxacademy.com/"}    },
	 UpdateExpression='SET visits = visits + :inc',
	 ExpressionAttributeValues={ ':inc': {'N': '1'}    },
	 ReturnValues="UPDATED_NEW")
     print("UPDATING ITEM")
     print(response)
   #+end_src
* DB
  - https://www.kdnuggets.com/2018/08/dynamodb-vs-cassandra.html
* AWS kafka
   - https://aws.amazon.com/msk/
   - https://www.quora.com/What-is-the-difference-between-Kafka-and-Spark
* AWS Spark
  - https://aws.amazon.com/big-data/what-is-spark/
* AWS redis
  - https://aws.amazon.com/elasticache/redis/
    


