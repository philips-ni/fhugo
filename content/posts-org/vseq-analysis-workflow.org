#+hugo_base_dir: ../../
# -*- mode: org; coding: utf-8; -*-
* Header Information                                               :noexport:
#+LaTeX_CLASS_OPTIONS: [11pt]
#+LATEX_HEADER: \usepackage{helvetica}
#+LATEX_HEADER: \setlength{\textwidth}{5.1in} % set width of text portion
#+LATEX_HEADER: \usepackage{geometry}
#+TITLE:     Vseq analysis workflow design
#+AUTHOR:    Fei Ni
#+EMAIL:     fei.ni@helix.com
#+DATE:      2021-10-19
#+HUGO_CATEGORIES: helix
#+HUGO_tags: helix
#+hugo_auto_set_lastmod: t
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:   ^:{}
#+INFOJS_OPT: view:nil toc:nil ltoc:nil mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+HTML_HEAD: <link rel="stylesheet" href="org.css" type="text/css"/>
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:

#+STARTUP: hidestars

#+STARTUP: overview   (or: showall, content, showeverything)
http://orgmode.org/org.html#Visibility-cycling  info:org#Visibility cycling

#+TODO: TODO(t) NEXT(n) STARTED(s) WAITING(w@/!) SOMEDAY(S!) | DONE(d!/!) CANCELLED(c@/!)
http://orgmode.org/org.html#Per_002dfile-keywords  info:org#Per-file keywords

#+TAGS: important(i) private(p)
#+TAGS: @HOME(h) @OFFICE(o)
http://orgmode.org/org.html#Setting-tags  info:org#Setting tags

#+NOstartup: beamer
#+NOLaTeX_CLASS: beamer
#+NOLaTeX_CLASS_OPTIONS: [bigger]
#+NOBEAMER_FRAME_LEVEL: 2


# Start from here

* Background/Rationale

 - Why is this project needed? 

Helix want to automate the viral sequencing workflow to enable sequencing and analysis.

 - Who will use it? 

After Vseq analysis result is created,  it will be pushed to =vseq review app=, so that some one from ?? team is able to review it.

 - When do we need it by? 

By End of Q4

 - What does MVP look like?
  - Vseq analysis pipeline  will be triggered after the Vseq Fastq data is ready from BSSH.
  - Vseq analysis result would be pushed to GLUE DB after Vseq pipeline is completed
  - Should not make impact for existing analysis workflow
  - Should support =VSeq-CAPTURE= and =VSeq-AMPLICON=
  - Be able to handle negative scenarios:
    - Failed to run some fasta generater batch job 
    - Failed to run klados lambda
    - Failed to call GLUE triggers
   
 - What will follow MVP?
   - Make Vseq analysis pipeline more robust
   - Make Vseq analysis pipeline running as fast as it can
   - Be able to track Vseq analysis pipeline's status in different stage

* Requirements

  Please see the PRD from https://myhelix.atlassian.net/wiki/spaces/PROD/pages/2422800494/Viral+Sequencing+workflow+PRD+DRAFT

* In Scope
  - fastq event listener which can be triggered after fastq data is ready
  - event handler which can identify if this event is for vseq, if yes, it may trigger the Vseq analysis pipeline
  - the vseq analysis pipeline itself

* Out of Scope
 - The =fastqToFasta generated batch job= ( It's owned by bioinformatics team, and it has been implemented)
 - The =klados= lambda ( It's owned by bioinformatics team, and it has been implemented)
 - The =vseq review app= ( It's the next project after this is done)

* Threat Modeling
Sometimes this section is optional depending on the Security

 

* Current Existing Workflow
Optionally describe and diagram what the current environment and workflow

 

* Solutions Considered

** Option 1: Doing nothing
*** Pros:
- no new development required
*** Cons:
- Require some manual work to trigger the pipeline and send back the result
- Require more manual work if some error happens while running vseq analsyis manually

** Option 2: Integrate everything into the current GENP platform

-  Create New step function for vseq analysis pipeline,  runs fastq -> fasta + klados and load result to GLUE
-  bcl2fastq-listener calls the new step function and 
-  use DDR for review
-  Everything still runs in master except for the fastq -> fastq conversion and klados


*** Pros:
- Reuse existing webhook/sns/sqs/bcl2fastqlistener in master account
- Reuse sample-metadata
- Reuse analsyis-workflow git repo


*** Cons:
- Need update existing analysis workflow,  increase the complexity, and there is regression risk
- Need update existing DDR, it also increase DDR's complexity as well.
- Cross-account(master->research) o/p to call =fastq -> fastq conversion and klados=

** Option 3: We build everything new in the hipaa zone, follow the pattern as current analysis-workflow is doing

 - New lambda that does what current =bcl2fastq-listener= does
 - Create batch/analysis objects in fastq-listener lambda, save them into DB and track their state in the whole process
 - New step function that runs fastq -> fasta + klados and load result to GLUE
 - New review tool (out of scope for this doc)


*** Pros:
- All new servies are in hipaa zone
- create batch/analysis records and store it in DB to align with current analysis-workflow
- If some sample's analysis failed,  it may cause the whole pipeline failed,  but we can resolve it by putting the failed fastqSessionId into sqs to trigger the whole flow again, and it will skip those analysis-completed samples.
- doesn't couple with analysis-workflow and existing lab API, no regression risk
- It's easy to bioinfomatics team to migrate their batch job, lambda to hipaa zone 

*** Cons:
-  A little complex comaring with Proposal2
-  Not exact the same as current manual process


** Option 4: We build everything new in the hipaa zone, follow the current manual process 
 - New lambda that does what current =bcl2fastq-listener= does
 - Do not create batch/analysis objects in fastq-listener lambda
 - New step function that runs fastq -> fasta + klados and load result to GLUE
 - New review tool (out of scope for this doc)


*** Pros
- replicate almost the same steps as  current manual process, simple, and quick be implemented quickly. 
- doesn't couple with analsyis-workflow and existing lab API, no regression risk
- all new services are in hipaa zone
- It's easy to bioinfomatics team to migreate their batch job, lambda to hipaa zone

*** Cons
- No batch/analysis records created as current analysis-workflow is doing, it's not easy to track each sample's  analsyis progress, status ,etc.
- if one sample's analysis failed,  it may require to rerun everything or require someone to do some dirty manual work to run failed ones and trigger submitGlueJobs manually 
* Data model

** refer to existing analysis workflow data model

 - batch/analyses structures
 - one fastqAppsessionId mapping to one runId/runName
 - fastqAppsessionid/runId, vseq pipeline version mapping to a VseqBatch
 - 1 VseqBatch can include 1000+ sample's VseqAnalyses


* Vseq core info

 - https://github.com/myhelix/helix-sars-klados
 - https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/stackinfo?stackId=arn%3Aaws%3Acloudformation%3Aus-east-1%3A409670809604%3Astack%2FHelixSarsKlados%2F18cd4aa0-ce1e-11eb-b7f1-0eaa1a5b8293

* SNS & SQS cross account integration
#+begin_src bash

Fei Ni Today at 1:36 PM
hey, any guidance/example about SNS & SQS cross-account integration? my use case is , SNS is in master account, SQS is in hipaa zone (edited) 

Chad Nicely  4 hours ago
fulfillment-workflow may provide some good examples - specifically the access policy on the SNS topic in master. Note the "subaccount-subscriptions" statement, which delegates permissions to principals (e.g. roles) in hipaa-production (032052122631):
{
  "Version": "2012-10-17",
  "Id": "__default_policy_ID",
  "Statement": [
    ...
    {
      "Sid": "subaccount-subscriptions",
      "Effect": "Allow",
      "Principal": {
        "AWS": [
          "arn:aws:iam::032052122631:root",
          "arn:aws:iam::424766430878:root"
        ]
      },
      "Action": [
        "SNS:Subscribe",
        "SNS:Receive"
      ],
      "Resource": "arn:aws:sns:us-east-1:820411415250:fulfillment-workflow-production"
    }
  ]
}

Chad Nicely  4 hours ago
@fei.ni are you wanting to do this in a CDK project?

Fei Ni  4 hours ago
ya

Fei Ni  4 hours ago
thanks @chad.nicely

Chad Nicely  4 hours ago
This particular project was provisioned with terraform monorepo, and takes advantage of "peer accounts" in the environments module. We may have a CDK example of this as well, but I need to have a look.

Chad Nicely  4 hours ago
Note that a corresponding permission would need to be added to the IAM policies of the role in hipaa-production.

John Corrales  3 hours ago
We do something similar for the custom resource that manages dns records Its from the early days of our cdk adoption, and not necessarily how i'd write it now, but probably still pretty close. one main difference is HERE where we hardcode the OU id, we can use this.namedEnv.organizationalUnit and it will resolve to the correct value in the correct place.
recordmanager-stack.ts
<https://github.com/myhelix/cfn-custom-resources|myhelix/cfn-custom-resources>myhelix/cfn-custom-resources | Added by GitHub
recordmanager-stack.ts
<https://github.com/myhelix/cfn-custom-resources|myhelix/cfn-custom-resources>myhelix/cfn-custom-resources | Added by GitHub

Fei Ni  3 hours ago
cool, thanks John!

John Corrales  3 hours ago
for the sqs side https://docs.aws.amazon.com/cdk/api/latest/docs/aws-sns-subscriptions-readme.html#amazon-sqs
docs.aws.amazon.comdocs.aws.amazon.com
@aws-cdk/aws-sns-subscriptions module Â· AWS CDK
Language | Package

John Corrales  3 hours ago
KMS key permissions will automatically be granted to SNS when a subscription is made to an encrypted queue.
:+1:
1


John Corrales  3 hours ago
Oh actually it looks like this.namedEnv.organizationalUnit doesn't resolve to the ou value, would probably have to use this instead, that mapping would be a good addition to cdk-library. maybe we can add that as part of the NamedEnv object as organizationalUnitId
organization.ts
class HelixOrganization {
<https://github.com/myhelix/cdk-library|myhelix/cdk-library>myhelix/cdk-library | Added by GitHub
:+1:
1


#+end_src

** calling webhook 

#+begin_src bash
curl -X POST 'https://bssh-events-webhook.staging.helix.com' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer <bearer token>' \
-d '{
	"dataType": "AppSessionStatus",
	"data": "{\"Comments\":\"Execution Status changed\",\"AppSessionId\":\"107199111\",\"AppName\":\"FASTQ Generation 2018-06-30 09:55:42Z\",\"ExecutionStatus\":\"Complete\"}"
}'
#+end_src
* reference 
 - https://github.com/myhelix/deletion
