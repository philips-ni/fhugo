
#+hugo_base_dir: ../../
# -*- mode: org; coding: utf-8; -*-
* Header Information                                               :noexport:
#+LaTeX_CLASS_OPTIONS: [11pt]
#+LATEX_HEADER: \usepackage{helvetica}
#+LATEX_HEADER: \setlength{\textwidth}{5.1in} % set width of text portion
#+LATEX_HEADER: \usepackage{geometry}
#+TITLE:     Running R2V
#+AUTHOR:    Fei Ni
#+EMAIL:     fei.ni@helix.com
#+DATE:      2021-01-01
#+HUGO_CATEGORIES: helix
#+HUGO_tags: helix
#+hugo_auto_set_lastmod: t
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+OPTIONS:   ^:{}
#+INFOJS_OPT: view:nil toc:nil ltoc:nil mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+HTML_HEAD: <link rel="stylesheet" href="org.css" type="text/css"/>
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:

#+STARTUP: hidestars

#+STARTUP: overview   (or: showall, content, showeverything)
http://orgmode.org/org.html#Visibility-cycling  info:org#Visibility cycling

#+TODO: TODO(t) NEXT(n) STARTED(s) WAITING(w@/!) SOMEDAY(S!) | DONE(d!/!) CANCELLED(c@/!)
http://orgmode.org/org.html#Per_002dfile-keywords  info:org#Per-file keywords

#+TAGS: important(i) private(p)
#+TAGS: @HOME(h) @OFFICE(o)
http://orgmode.org/org.html#Setting-tags  info:org#Setting tags

#+NOstartup: beamer
#+NOLaTeX_CLASS: beamer
#+NOLaTeX_CLASS_OPTIONS: [bigger]
#+NOBEAMER_FRAME_LEVEL: 2


# Start from here
* command
 - r2v --platform host --json s3://304674702989-hipaa-exome-workflow/test/fei/fei_test_r2v_run_cfg.json
 - ["r2v","--platform","host","--json","s3://820411415250-hipaa-exome-workflow/test_input/config.json"]  (in master account)
* job queue
 - arn:aws:batch:us-east-1:820411415250:job-queue/hipaa-exome-workflow-staging-job-queue (in master account)
 - hipaa-exome-workflow-job-queue

* computing environment
 - BatchComputeEnvDA54D2E3-b0ef27b0c7581fd

* config file (input.json)

#+begin_src json
{
  "config": {
    "alignment_input.remote_fastq": [
       "s3://304674702989-hipaa-exome-workflow/test/yunyun/input/LP0005765-DNA-C12-6c9be09e-01ff-44ad-b6cf-d34244693c3c_S35_L001_R1_001.fastq.gz",
       "s3://304674702989-hipaa-exome-workflow/test/yunyun/input/LP0005765-DNA-C12-6c9be09e-01ff-44ad-b6cf-d34244693c3c_S35_L001_R2_001.fastq.gz"
    ], # this is a list of input presigned URLs
    "analysis.sample": "test_sample",  # output files will use this as prefix
    "analysis.assay_version": "v3",
    "analysis.tag": "container",
    "scratch.scratch_dir": "/scratch",
    "reference_bundle.basepath": "/reference/2.14.0/ref_bundle",
    "output.output_dir": "/scratch/output",
    "logs.logs_dir": "/scratch/output/logs",
    "output.s3_upload_uri": "s3://304674702989-hipaa-exome-workflow/r2v_test" # this is your output dir, the job role hipaa staging has access to this bucket, you should put this JSON file in this bucket as well
  }
}
#+end_src

* Current Data flow

** BSSH  calling bssh-events-webhook (Restful Call)
 - https://console.aws.amazon.com/sns/v3/home?region=us-east-1#/topic/arn:aws:sns:us-east-1:820411415250:bssh-events-webhook-staging
#+begin_src bash

// Event Receiving End
{
	"id": "bf3d001d-05a7-4a35-b023-bbd8c1670d5f",
	"receivedTimestamp": "2015-05-19T07:31:23+00:00",
	"dataType": "someEventPayloadType",
	"data":"<messageBody or payload>"
}



#+end_src

** bssh-events-webhook to SNSTopicARN to SQSArn
 - arn:aws:sns:us-east-1:820411415250:bssh-events-webhook-staging
 - arn:aws:sqs:us-east-1:820411415250:bcl2fastq-listener-bssh-events-staging
 - arn:aws:sqs:us-east-1:820411415250:ddr-listener-sqs-staging
 - arn:aws:sqs:us-east-1:820411415250:sample-mapping-service-sqs-staging
 - arn:aws:sqs:us-east-1:820411415250:ddr-bssh-events-staging

#+end_src

** bcl2fastq-listener pulling from SQS
#+begin_src bash
{
  "correlationId": "352bb7a7-0ebe-4d46-be7e-466a226fdef6",
  "data": {
    "AppSessionID": "437872436",
    "AppName": "FASTQ Generation",
    "ExecutionStatus": "Complete",
    "QcStatus": "Undefined",
    "DeliveryStatus": "None",
    "Comments": "Execution Status changed",
    "AllowLegacyAppSession": false,
    "BypassReview": false,
    "ForceReanalysis": false
  },
  "event": "basespace/event",
  "level": "info",
  "msg": "AppSession Event received",
  "reportedAt": "/go/src/github.com/myhelix/bcl2fastq-listener/lib/sqs/event_handler.go:114",
  "time": "2021-07-11T18:36:12.860514086Z"
}


#+end_src

** bcl2fastq-listener create batch and sent to step functions

batch may include batchID and a list of AnalysisIds
 - arn:aws:states:us-east-1:820411415250:stateMachine:analysis-workflow-staging-cnv-state-machine-dp
#+begin_src bash
func createBatchesFromAppSessionData(
        ctx context.Context,
        labClient *client.LabClient,
        data *input.AppSessionData) ([]lab.Batch, error) {

        return labClient.Analysis.CreateBatches(ctx, &lab.CreateBatchInput{
                Bcl2FastqId:           data.AppSessionID,
                AllowLegacyAppSession: data.AllowLegacyAppSession,
                BypassReview:          data.BypassReview,
                ForceReanalysis:       data.ForceReanalysis,
        })
}
#+end_src
=labClient.Analysis.CreateBatch= will do this:
 - calling  =makeBatches= to create required batch as input for steps function


#+begin_src bash
func (s *DefaultAnalysisService) makeBatches(ctx context.Context, input *lab.CreateBatchInput) ([]lab.Batch, error) {
        defer lab.TimeTrack(ctx, time.Now(), "makeBatches")
        usePool := input.PoolId
        // Create batch and all analysis with it
        run, err := s.labClient.Fastq.GetFastqAppSessionRun(input.Bcl2FastqId)
        if err != nil {
                return nil, err
        }
        // This could take a looong time
        lanes, err := s.labClient.Run.GetLibraryLanesWhenDatasetsUploaded(run.Id)
        if err != nil {
                return nil, err
        }

        poolMap, err := s.mkBatchesGroupedByPoolId(ctx, input, run.Id, lanes)
        if err != nil {
                return nil, err
        }

        // flatten the map: return a list of batches where each batch has a unique pool id
        var batchHolders []*batchHolder
        var batches []lab.Batch
        for poolId, bh := range poolMap {
                if usePool == "" || usePool == poolId {
                        batchHolders = append(batchHolders, bh)
                        batches = append(batches, *bh.batch)
                }
        }

        // Persist the batches
        // each batch holder has a batch and the analyses of that batch
        err = s.persistBatches(ctx, batchHolders)
        if err != nil {
                return nil, err
        }
        return batches, nil
}

#+end_src

The final batch input format to step functions is :
#+begin_src bash
type Batch struct {
        BatchId               string                     `protobuf:"bytes,1,opt,name=BatchId" json:"BatchId,omitempty"`
        Bcl2FastqId           string                     `protobuf:"bytes,2,opt,name=Bcl2FastqId" json:"Bcl2FastqId,omitempty"`
        PoolId                string                     `protobuf:"bytes,3,opt,name=PoolId" json:"PoolId,omitempty"`
        LimsPoolId            string                     `protobuf:"bytes,4,opt,name=LimsPoolId" json:"LimsPoolId,omitempty"`
        PoolName              string                     `protobuf:"bytes,5,opt,name=PoolName" json:"PoolName,omitempty"`
        RunId                 string                     `protobuf:"bytes,6,opt,name=RunId" json:"RunId,omitempty"`
        FlowcellId            string                     `protobuf:"bytes,7,opt,name=FlowcellId" json:"FlowcellId,omitempty"`
        CreationTime          *google_protobuf.Timestamp `protobuf:"bytes,8,opt,name=CreationTime" json:"CreationTime,omitempty"`
        CompletionTime        *google_protobuf.Timestamp `protobuf:"bytes,9,opt,name=CompletionTime" json:"CompletionTime,omitempty"`
        AnalysisIds           []string                   `protobuf:"bytes,10,rep,name=AnalysisIds" json:"AnalysisIds,omitempty"`
        CnvInput              *BatchJobInput             `protobuf:"bytes,11,opt,name=CnvInput" json:"CnvInput,omitempty"`
        Status                Status                     `protobuf:"varint,12,opt,name=Status,enum=lab.Status" json:"Status,omitempty"`
        AllowLegacyAppSession bool                       `protobuf:"varint,13,opt,name=AllowLegacyAppSession" json:"AllowLegacyAppSession,omitempty"`
        BypassReview          bool                       `protobuf:"varint,14,opt,name=BypassReview" json:"BypassReview,omitempty"`
        MetadataJson          *OutputFile                `protobuf:"bytes,15,opt,name=MetadataJson" json:"MetadataJson,omitempty"`
        ReanalysisRequested   bool                       `protobuf:"varint,16,opt,name=ReanalysisRequested" json:"ReanalysisRequested,omitempty"`
        ExecutionArn          string                     `protobuf:"bytes,17,opt,name=ExecutionArn" json:"ExecutionArn,omitempty"`
}
#+end_src

BTW, related Analysis would be created as part of this process:
makeBatches -> mkBatchesGroupedByPoolId -> createAnalysisForSample -> createAnalysis

#+begin_src bash
func (s *DefaultAnalysisService) createAnalysis(ctx context.Context, input *lab.CreateAnalysisInput) (*lab.Analysis, error) {
        defer lab.TimeTrack(ctx, time.Now(), "createAnalysis")
        analysisId := uuid.UuidString(uuid.ANALYSIS)
        sample, err := s.labClient.BioSample.GetSample(input.BioSampleId)
        if err != nil {
                return nil, err
        }

        analysis := &lab.Analysis{                                                                                                                                                                                                                                                                                                                     Fill lab.Analysis
                AnalysisId:          analysisId,
                BatchId:             input.BatchId,
                BioSampleId:         input.BioSampleId,
                SampleId:            sample.Name,
                LibraryName:         input.LibraryName,
                ReanalysisRequested: input.ReanalysisRequested,
                EnrichmentPoolId:    input.EnrichmentPoolId,
                LibraryPlateId:      input.LibraryPlateId,
                Bcf: &lab.OutputFile{
                        Path: s.createS3Path(fileTypeBcf, input.BatchId, analysisId),
                },
                BcfIndex: &lab.OutputFile{
                        Path: s.createS3Path(fileTypeBcfIndex, input.BatchId, analysisId),
                },
                SampleQc: &lab.OutputFile{
                        Path: s.createS3Path(fileTypeQc, input.BatchId, analysisId),
                },
                R2VAutoQcValues: &lab.OutputFile{
                        Path: s.createS3Path(fileTypeR2VAutoQcMetrics, input.BatchId, analysisId),
                },
                SampleCnvCalls: &lab.OutputFile{
                        Path: s.createS3FilePath(fileTypeCnv, input.BatchId, analysisId, fmt.Sprintf("%s.cnv.results.txt", sample.Name)),
                },
                SampleCnvMetrics: &lab.OutputFile{
                        Path: s.createS3FilePath(fileTypeCnvMetrics, input.BatchId, analysisId, fmt.Sprintf("%s.cnv.qc.json", sample.Name)),
                },
                R2VAutoQcComparisonResults: &lab.OutputFile{
                        Path: s.createS3FilePath(fileTypeR2VAutoQcMetrics, input.BatchId, analysisId, fmt.Sprintf("%s.r2v.autoqc.comparison.pb", sample.Name)),
                },
        }

        // This should still call BSSH for the latest app session id as we can still run r2v directly on BSSH
        err = s.refreshAnalysis(analysis, false)
        if err != nil {
                return nil, err
        }

        return analysis, nil
}
#+end_src

** step functions calling SubmitR2V

#+begin_src bash
func SubmitR2V(ctx context.Context, data json.RawMessage) (interface{}, error) {
        var analysisId string
        err := json.Unmarshal(data, &analysisId)
        log.Debug(fmt.Sprintf("SubmitR2V %s", string(data[:])))
        if err != nil {
                return data, merry.Wrap(err)
        }
        c := service.GetLabClient()
        analysis, err := c.Analysis.GetAnalysisSkipBSSHForLatestAppSession(ctx, analysisId)
        if err != nil {
                return nil, merry.Wrap(err)
        }

        err = c.Analysis.RerunAnalysis(ctx, analysis)
        if err != nil {
                return nil, merry.Wrap(err)
        }

        if analysis.R2VExecutionCount == 1 {
                logger := log.FromContext(ctx)
                audit.LogAnalysisR2VInitiated(logger, analysisId)
        }

        return nil, nil
}
#+end_src

** step functions calling WaitR2V
 - send request to BSSH to check if R2V job completed or not(need be changed)
 - WaitR2V is just calling GetR2V to get R2V's latest status and put it in a look until it's completed

* New Data Flow

new bcl2fastq listener logic

 -  Set =analysis.CompletionStatus = lab.CompletionStatus_BioSampleCancelled= when sample's status is cancelled (same as current implementation)
 -  Set =analysis.CompletionStatus = lab.CompletionStatus_MissingYield when sample's status is missingYield  (same as current implementation)
 -  Set =analysis.CompletionStatus = lab.CompletionStatus_NoJob= when querying metadata table, cannot find entry for given sampleID + r2v_version (new)
 -  add new logic to determine r2v_version/pipelineVersion based on sample's assay_version
 -  Set =analysis.CompletionStatus = lab.CompletionStatus_JobPending= when querying metadata table, find r2vJobID for given sampleID + r2v_version but it's still running (new)
 -  Set =analysis.CompletionStatus = lab.CompletionStatus_JobFailed= when querying metadata table, find r2vJobID for given sampleID + r2v_version but it's failed (new)
 -  Set =analysis.CompletionStatus = lab.CompletionStatus_JobSucceed= when querying metadata table, find r2vJobID for given sampleID + r2v_version and it succeeded (new)

** Query R2V status
  
* Questions
 - What's the concept of appsession? in which condition a appsession would be created? and who(which process) create it? is it BSSH?
   appsession is a session for a BSSH analysis app, for example, while doing bcl2fastq, a appsession will be created, while doing r2v, another appsession will be created.
 - r2v version/pipelineVersion, assay_version
   - when assay_version is v3, we should use latest r2v version(helix_r2v_validation_v4.3.0-WF1.0.0)
   - when assay_version is less than v3, we should use lower version r2v version (helix_r2v_v4.0.2-WF1.0.0)
#+begin_src bash
var productIdToAssayVersion = map[string]string{
        "20010958_20010959":           "v1",
        "100956_100957_100958_100959": "v2",
        "100956_100535_100958_100959": "v3",
}
#+end_src
   - pipelineVersion is r2v version
#+begin_src bash
// GetPipelineVersion will parse the semantic pipeline version that corresponds to an AppSession
// processed by Helix's variant calling pipeline. The pipeline name is accessible as a component
// of the AppSession entity at AppSession.Application.Name. The version at AppSession.Application.Version
// is not useful for our purposes as it instead corresponds to the version of the workflow that
// was generated from the pipeline.
// This function depends on a pre-determined formatting of the pipeline name as:
// -- helix_r2v_v{major}.{minor}.{revision}
func (s *BasespaceInternal) GetPipelineVersion(appSessionId string) (string, error) {
        appSession, err := s.GetAppSession(appSessionId)
        if err != nil {
                return "", err
        }
        return appSession.Application.Version()
}
#+end_src
 - Which process will update batch's status? and when?
   - =CompleteBatch and FailBatch= lambda will be used to update batch's status, which is part of step functions

 - How to get labconfig detail?
  #+begin_src bash
labConfig, err := metadataClient.GetLabConfig(context.Background())
...
func (c *DefaultClient) getLabConfigUrl() string {
        return c.Url + "/config/lab_config"
}
# from sample-metadata v114
[fei.ni@fei-ni-C02FG3R2MD6N-SM master-dev myhelix]$ curl -H 'X-Auth-Token-Internal: IS-IDT732R33OK24T2Q6HDKGSVHJ4HNP6D4' -H 'X-Auth-Caller-Internal: analysis-workflow' -H 'Content-Type: application/json' https://sample-metadata.staging.helix.com/v0/config/lab_config |jq .
{
  "CnvJobQueue": "analysis-workflow-job-queue-staging",
  "PgxJobQueue": "analysis-workflow-pgx-job-queue-staging",
  "ResultsBucket": "helix-analysis-workflow-results-staging",
  "AncestryJobQueue": "prw-ancestry-job-queue-staging",
  "CnvJobDefinition": "helix-py-app-cnv.staging.helix.com",
  "PgxJobDnsEndpoint": "helix-py-app-pgx.staging.helix.com",
  "SimpleTraitsJobQueue": "trait-compute-batch-job-queue-staging",
  "AncestryJobDefinition": "helix-py-app-ancestry.staging.helix.com",
  "SimpleTraitsDnsEndpoint": "trait-compute-batch.staging.helix.com",
  "RegionalAncestryJobQueue": "prw-regionalAncestry-job-queue-staging",
  "CompmetricLambdaDefinition": "helix-py-app-compmetric.staging.helix.com",
  "RegionalAncestryJobDefinition": "helix-py-app-regional-ancestry.staging.helix.com",
  "BioinformaticsCurrentApplication": "helix_r2v_validation_v4.3.0-WF1.0.0"
}

  #+end_src

  - How to access sample-metadata
     - https://sample-metadata.staging.helix.com/v0
#+begin_src bash
[fei.ni@fei-ni-C02FG3R2MD6N-SM master-dev myhelix]$ hops secret get -e staging -s analysis-workflow -p internal-service/sample-metadata/api-key/apiKey
IS-IDT732R33OK24T2Q6HDKGSVHJ4HNP6D4

# curl -H 'X-Auth-Token-Internal: IS-IDT732R33OK24T2Q6HDKGSVHJ4HNP6D4' -H 'X-Auth-Caller-Internal: analysis-workflow' -H 'Content-Type: application/json' https://sample-metadata.staging.helix.com/v0/config/lab_config
# hops db config -f eval -e staging -s sample-metadata
PGPASSWORD='zqtX2wVjzfcBtGcQGucMLJek' psql --no-password 'dbname='samplemetadata' user='sample-metadata-service' password='zqtX2wVjzfcBtGcQGucMLJek' host='staging-samplemetadata.cluster-crbiutp3k1kf.us-east-1.rds.amazonaws.com' port=5432 sslmode='require''

samplemetadata=> select * from metadata where metadata_type = 'analysis' and raw_json->>'BioSampleId' = '124614584'
|  AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3 | analysis      | {"Bai": null, "Bam": null, "Bcf": {"Path": "s3://helix-analysis-workflow-results-staging/bcf/BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY/AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3/SA-14-Plate_1-F11_partner_9437428.201465318_helix.bcf", "Version": "umZ097l7ppXwG66Rde450bbX_qye57YM"}, "Yield": "8163753408", "QcPass": true, "BatchId": "BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY", "BcfIndex": {"Path": "s3://helix-analysis-workflow-results-staging/bcf/BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY/AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3/SA-14-Plate_1-F11_partner_9437428.201465318_helix.bcf.csi", "Version": "gA24iI27ZfDbzFuVVg5gQhXAs.8FxdbC"}, "SampleId": "SA-14-Plate_1-F11", "SampleQc": {"Path": "s3://helix-analysis-workflow-results-staging/qc/BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY/AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3/SA-14-Plate_1-F11_qctable_9437428.201465318_helix.txt", "Version": "Ynj2hoiBpUJeA6BofORLlGlbT6ef9oXN"}, "AnalysisId": "AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3", "BioSampleId": "124614584", "LibraryName": "SA-14-Plate_1-F11", "AppSessionId": "201478287", "AssayVersion": "v2", "CreationTime": "2020-01-27T20:40:51.000Z", "AncestryInput": null, "CompletionTime": "2020-01-27T22:55:06.000Z", "LibraryPlateId": "", "SampleCnvCalls": {"Path": "s3://helix-analysis-workflow-results-staging/cnv/BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY/AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3/SA-14-Plate_1-F11.cnv.results.txt", "Version": "xDRWCc4MIVj_CGwYdGlCfzgCG8FOq0Rw"}, "MayoMetricsPass": false, "PipelineVersion": "4.0.0", "R2VAutoQcValues": {"Path": "s3://helix-analysis-workflow-results-staging/helix-py-app-compmetric/BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY/AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3/dataset.json", "Version": "EavHXlcuNJF.JxjdU6kXslEx02s4dNIL"}, "CompletionStatus": "Complete", "EnrichmentPoolId": "", "IsCnvDeliverable": false, "ModificationTime": "2020-01-27T22:56:44.000Z", "PostReviewStatus": "Initialized", "SampleCnvMetrics": {"Path": "s3://helix-analysis-workflow-results-staging/cnvMetrics/BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY/AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3/SA-14-Plate_1-F11.cnv.qc.json", "Version": "k7fR8_R.4QHZWDzmDTL1BnU_HaWdABdo"}, "R2VExecutionCount": "0", "CnvPipelineVersion": "", "MayoMetricsVersion": "", "ReanalysisRequested": false, "SampleQcTableByRegion": null, "PostReviewExecutionArn": "", "ContinentalAncestryOutput": null, "R2VAutoQcComparisonResults": {"Path": "s3://helix-analysis-workflow-results-staging/helix-py-app-compmetric/BA-5BMBRUFWMPT7KAFQAUVZ3UF5WA6JTADY/AN-VMI2OVIYO6CV5YI3F6HICQY3K6XCIFV3/SA-14-Plate_1-F11.r2v.autoqc.comparison.pb", "Version": ""}, "ContinentalAncestryAlgorithmVersion": ""}                                                                                                                                            ;
#+end_src
* TODO
 - Figure out the way to get fastq file path by querying with sampleID, r2v_version, fastq appSessionID (DONE)
 - How does CNV/PGX analysis use those outpuf from R2V job?
 - How do we know if a analysis is canceled/disabled/stuck/missingYield?

* User cases

there are 3 user cases to trigger analysis-workflow
  - New sample's sequencing (need run everything) 
  - Requeue existing sample, which means reduo the sequencing for given sample (need run everything)
  - Reanalysis (it may skip those analysis which have been done)


Question: How to differentiate =Requeue=  and =Reanalysis?=

#+begin_src bash
// in bcl2fastq-listener/router/router.go
// this is for reanalysis
func versionedRoutes(r *gin.RouterGroup, v int) {
        basespace_events.Register(v, r.Group("basespaceevents"))  
        reanalyze.Register(v, r.Group("reanalyze")) 
}
...
func handleReanalyze(ctx log.ContextLogger, event *ReanalyzeRequest) ([]*statemachine.BatchExecutionStartOutput, error) {
        labClient, err := helpers.NewLabClient()
        if err != nil {
                return nil, err
        }
        batch, err := labClient.Analysis.CreateBatch(ctx, &lab.CreateBatchInput{
                PoolId:               event.PoolId,
                RunId:                event.RunId,
                ReanalyzedBioSamples: event.BioSampleIds,
        })
        if err != nil {
                return nil, merry.Prepend(err, "Error creating batch").WithValue("event", event)
        }

        smExec := &statemachine_lib.DefaultExecutor{sfn.New(aws.GetOrCreateDefaultSession())}
        dispatcher := statemachine.NewStateMachineDispatcher(smExec, labClient, config.GetStateMachineArn())
        return dispatcher.KickOffStateMachines(ctx, []lab.Batch{*batch})
}

// bcl2fastq-listener/main.go
// this is for new sequencing and requeue
sqs.StartPoller(
  context.Background(),
  sqs.DefaultBsshPollerConfig(),
)
...
func createBatchesFromAppSessionData(
        ctx context.Context,
        labClient *client.LabClient,
        data *input.AppSessionData) ([]lab.Batch, error) {

        return labClient.Analysis.CreateBatches(ctx, &lab.CreateBatchInput{
                Bcl2FastqId:           data.AppSessionID,
                AllowLegacyAppSession: data.AllowLegacyAppSession,
                BypassReview:          data.BypassReview,
                ForceReanalysis:       data.ForceReanalysis,
        })
}
#+end_src

please notice, one is calling =labClient.Analysis.CreateBatch=, another one is calling =labClient.Analysis.CreateBatches=
the difference is :
 - CreateBatches will pass Bcl2FastqId
 - CreateBatch will get Bcl2FastqId by querying with runId
 - CreateBatch also pass =ReanalyzedBioSamples= for those samples, we will set "samplesForReanalysis[bioSampleId] = true" in mkBatchesGroupedByPoolId, and it will be passed to s.createAnalysisForSample
 - both of them calling createBatches

** What does createBatches do?
  - Get Run from Bcl2FastqId 
  - Get Lanes from run.id 
   - lanes, err := s.labClient.Run.GetLibraryLanesWhenDatasetsUploaded(run.Id)
   - It will keep trying until fastq datasets are ready 
  - calling mkBatchesGroupedByPoolId to get poolMap
   - It Generates batched based on the information inside the lanes
   - Typically there are 2 pools in 1 bcl2fastq app session, and 48 samples (for Exome+ v1) or 72 samples (for Exome+ v2) in 1 pool
   - so totally, we have 72 X 2 = 144 samples in 1 bcl2fastq app session
   - poolMap, err := s.mkBatchesGroupedByPoolId(ctx, input, run.Id, lanes)
    - it create batch for each pool
   
  
* Others
- How to get presignedURL for a given sampleID
  - sampleID -> Datasets (bsClient.BioSample.GetSampleFastqDatasets(sampleId))
  - DatasetID -> *lab.Files (func (s *BasespaceInternal) GetDatasetFiles)
  - fileID -> URL (func (s *BasespaceInternal) GetPresignedUri(baseSpaceFileId string))
  - https://github.com/myhelix/ddr/pull/124/files
  
- Will new sequencing, requeue, reanalysis, for each of them, even for the same sample, different/uniq analysisId will be created?  YES
- Very important function =convertAppSessionToAnalysis=


** dev only code 
#+begin_src golang
        /* for development only start
        samplesWhiteList := map[string]bool{
                "206803716": true,
                "206810781": true,
        }
        if _, foundIt := samplesWhiteList[analysisObj.BioSampleId]; !foundIt {
                logger.Info(fmt.Sprintf("Skip R2V run for %s only for test purpose", analysisObj.BioSampleId))
                return states.R2V_Exit, nil
        }
        logger.Info(fmt.Sprintf("checking %v", analysisObj))
        for development only end
        */
#+end_src
